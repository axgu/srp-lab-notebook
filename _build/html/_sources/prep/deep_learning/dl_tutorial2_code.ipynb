{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-47974a2c56a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Data retrieval and loading\n",
    "\n",
    "import hashlib\n",
    "import requests\n",
    "fname = \"W3D4_stringer_oribinned6_split.npz\"\n",
    "url = \"https://osf.io/p3aeb/download\"\n",
    "expected_md5 = \"b3f7245c6221234a676b71a1f43c3bb5\"\n",
    "\n",
    "if not os.path.isfile(fname):\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "    except requests.ConectionError:\n",
    "        print(\"!!! Failed to download data !!!\")\n",
    "    else:\n",
    "        if r.status_code != requests.codes.ok:\n",
    "            print(\"!!! Failed to download data !!!\")\n",
    "        elif hashlib.md5(r.content).hexdigest() != expected_md5:\n",
    "            print(\"!!! Data download appears corrupted !!!\")\n",
    "        else:\n",
    "            with open(fname, \"wb\") as fid:\n",
    "                fid.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Plotting Functions\n",
    "\n",
    "def show_stimulus(img, ax=None):\n",
    "    \"\"\"Visualize a stimulus\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.imshow(img+0.5, cmap = mpl.cm.binary)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "\n",
    "def plot_weights(weights, channels=[0], colorbar=True):\n",
    "    \"\"\" plot convolutional channel weights\n",
    "    Args:\n",
    "        weights: weights of convolutional filters (conv_channels x K x K)\n",
    "        channels: which conv channels to plot\n",
    "    \"\"\"\n",
    "    wmax = torch.abs(weights).max()\n",
    "    fig, axs = plt.subplots(1, len(channels), figsize=(12, 2.5))\n",
    "    for i, channel in enumerate(channels):\n",
    "        im = axs[i].imshow(weights[channel, 0], vmin=-wmax, vmax=wmax, cmap='bwr')\n",
    "        axs[i].set_title('channel %d'%channel)\n",
    "    \n",
    "    if colorbar:\n",
    "        ax = fig.add_axes([1, 0.1, 0.05, 0.8])\n",
    "        plt.colorbar(im, ax=ax)\n",
    "        ax.axis('off')\n",
    "\n",
    "def plot_example_activations(stimuli, act, channels=[0]):\n",
    "    \"\"\" plot activations act and corresponding stimulus\n",
    "    Args:\n",
    "        stimuli: stimulus input to convolutional layer (n x h x w) or (h x w)\n",
    "        act: activations of convolutional layer (n_bins x conv_channels x n_bins)\n",
    "        channels: which conv channels to plot\n",
    "    \"\"\"\n",
    "    if stimuli.ndim>2:\n",
    "        n_stimuli = stimuli.shape[0]\n",
    "    else:\n",
    "        stimuli = stimuli.unsqueeze(0)\n",
    "        n_stimuli = 1   \n",
    "\n",
    "    fig, axs = plt.subplots(n_stimuli, 1+len(channels), figsize=(12,12))\n",
    "\n",
    "    # plot stimulus\n",
    "    for i in range(n_stimuli):\n",
    "        show_stimulus(stimuli[i].squeeze(), ax=axs[i, 0])\n",
    "        axs[i, 0].set_title('stimulus')\n",
    "\n",
    "        # plot example activations\n",
    "        for k, (channel, ax) in enumerate(zip(channels, axs[i][1:])):\n",
    "            img=ax.imshow(act[i,channel], vmin=-3, vmax=3, cmap='bwr')\n",
    "            ax.set_xlable('x-pos')\n",
    "            ax.set_ylabel('y-pos')\n",
    "            ax.set_title('channel %d'%channel)\n",
    "    ax = fig.add_axes([1.05, 0.8, 0.01, 0.1])\n",
    "    plt.colorbar(img, cax=ax)\n",
    "    ax.set_title('activation\\n strength')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Helper Functions\n",
    "\n",
    "def load_data_split(data_name=fname):\n",
    "    \"\"\"Load mouse V1 data from Stringer et al. (2019)\n",
    "\n",
    "    Data from study reported in this preprint: https://www.biorxiv.org/content/10.1101/679324v2.abstract\n",
    "\n",
    "    These data comprise time-averaged responses of ~20,000 neurons to ~4,000 stimulus gratings of different orientations, recorded\n",
    "    through Calcium imaginge. The responses have been normalized by spontaneous levels of activity and then z-scored over stimuli, so\n",
    "    expect negative numbers. The repsonses were split into train and test and then each set were averaged in bins of 6 degrees.\n",
    "\n",
    "    This function returns the relevant data (neural responses and stimulus orientations) in a torch.Tensor of data type torch.float32\n",
    "    in order to match the default data type for nn.Parameters in Google Colab.\n",
    "\n",
    "    It will hold out some of the trials when averaging to allow us to have test tuning curves.\n",
    "\n",
    "    Args:\n",
    "        data_name (str): filename to load\n",
    "\n",
    "    Returns:\n",
    "        resp_train (torch.Tensor): n_stimuli x n_neurons matrix of neural responses, each row contains the responses of each neuron to a given stimulus.\n",
    "            As mentioned above, neural \"response\" is actually an average over responses to stimuli with similar angles falling within specified bins.\n",
    "        resp_test (torch.Tensor): n_stimuli x n_neurons matrix of neural responses, each row contains the responses of each neuron to a given stimulus.\n",
    "            As mentioned above, neural \"response\" is actually an average over responses to stimuli with similar angles falling within specified bins\n",
    "        stimuli: (torch.Tensor): n_stimuli x 1 column vector with orientation of each stimulus, in degrees. This is actually the mean orientation of all stimuli in each bin.\n",
    "\n",
    "    \"\"\"\n",
    "    with np.load(data_name) as dobj:\n",
    "        data = dict(**dobj)\n",
    "    resp_train = data['resp_train']\n",
    "    resp_test = data['resp_test']\n",
    "    stimuli = data['stimuli']\n",
    "\n",
    "    # Return as torch.Tensor\n",
    "    resp_train_tensor = torch.tensor(resp_train, dtype=torch.float32)\n",
    "    resp_test_tensor = torch.tensor(resp_test, dtype=torch.float32)\n",
    "    stimuli_tensor = torch.tensor(stimuli, dtype = torch.float32)\n",
    "\n",
    "    return resp_train_tensor, resp_test_tensor, stimuli\n",
    "\n",
    "def filters(out_channels=6, K=7):\n",
    "    \"\"\" make example filters, some center-surround and gabors\n",
    "    Returns:\n",
    "    filters: out_channels x K x K\n",
    "    \"\"\"\n",
    "    grid = np.linspace(-K/2, K/2, K).astype(np.float32)\n",
    "    xx,yy = np.meshgrid(grid, grid, indexing='ij')\n",
    "\n",
    "    # create center-surround filters\n",
    "    sigma = 1.1\n",
    "    gaussian = np.exp(-(xx**2 + yy**2)**0.5 / (2*sigma**2))\n",
    "    center_surround = gaussian - 0.5 * wide_gaussian\n",
    "\n",
    "    # create gabor filters\n",
    "    thetas = np.linspace(0, 180, out_channels-2+1)[:-1] * np.pi/180\n",
    "    gabors = np.zeros((len(thetas), K, K), np.float32)\n",
    "    lam = 10\n",
    "    phi = np.pi/2\n",
    "    gaussian = np.exp(-(xx**2 + yy**2)**0.5/(2*(sigma*0.4)**2))\n",
    "    for i, theta in enumerate(thetas):\n",
    "        x = xx*np.cos(theta) + yy*np.sin(theta)\n",
    "        gabors[i] = gaussian * np.cos(2*np.pi*x/lam + phi)\n",
    "    \n",
    "    filters = np.concatenate((center_surround[np.newaxis,:,:],-1*center_surround[np.newaxis,:,:],gabors), axis = 0)\n",
    "    filters /= np.abs(filters).max(axis=(1,2))[:,np.newaxis,np.newaxis]\n",
    "    filters -= filters.mean(axis=(1,2))[:, np.newaxis,np.newaxis]\n",
    "    # convert to torch\n",
    "    filters = torch.from_numpy(filters)\n",
    "    # add channel axis\n",
    "    filters = filters.unsqueeze(1)\n",
    "\n",
    "    return filters\n",
    "\n",
    "def grating(angle, sf=1 / 28, res=0.1, patch=False):\n",
    "    \"\"\"Generate oriented grating stimulus\n",
    "\n",
    "    Args:\n",
    "        angle (float): orientation of grating (angle from vertical), in degrees\n",
    "        sf (float): controls spatial frequency of the grating\n",
    "        res (float): resolution of image. Smaller values will make the image\n",
    "        smaller in terms of pixels. res=1.0 corresponds to 640 x 480 pixels.\n",
    "        patch (boolean): set to True to make the grating a localized patch on the left side of the image. If False, then the grating occupies the full image.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: (res * 480) x (res * 640) pixel oriented grating image\n",
    "    \"\"\"\n",
    "\n",
    "    angle = np.deg2rad(angle)   # transform to radians\n",
    "\n",
    "    wpix, hpix = 640, 480    # width and height of image in pixels for res=1.0\n",
    "\n",
    "    xx, yy = np.meshgrid(sf * np.arange(0, wpix * res) / res, sf * np.arange(0, hpix * res) / res)\n",
    "\n",
    "    if patch:\n",
    "        gratings = np.cos(xx * np.cos(angle + .1) + yy * np.sin(angle + .1))  # phase shift to make it better fit within patch\n",
    "        gratings[gratings < 0] = 0\n",
    "        gratings[gratings > 0] = 1\n",
    "        xcent = gratings.shape[1] * .75\n",
    "        ycent = gratings.shape[0] / 2\n",
    "        xxc, yyc = np.meshgrid(np.arange(0, gratings.shape[1]), np.arange(0, gratings.shape[0]))\n",
    "        icirc = ((xxc - xcent) ** 2 + (yyc - ycent) ** 2) ** 0.5 < wpix / 3 / 2 * res\n",
    "        gratings[~icirc] = 0.5\n",
    "\n",
    "    else:\n",
    "        gratings = np.cos(xx * np.cos(angle) + yy * np.sin(angle))\n",
    "        gratings[gratings < 0] = 0\n",
    "        gratings[gratings > 0] = 1\n",
    "\n",
    "    gratings -= 0.5\n",
    "\n",
    "    # Return torch tensor\n",
    "    return torch.tensor(gratings, dtype=torch.float32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.2 32-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b00ea486395756e1e0d6d59adc8a698c93f0efe761447640c85f15a5a870479b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
