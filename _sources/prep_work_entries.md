# Prep Work
<b>Deep Learning Intro</b><br>
<i>06/01/2022, 7:45 am</i>


Synopsis: I will work through the Deep Learning Intro module.

Data: I watched the introduction video of the Deep Learning module I am following. The video covered the basics of convolutional neural networks, how they work, and visualizing them. Since the module is related to computational neuroscience, I also learned about performing analyses with fMRI data. The video also discussed comparisons between artificial and natural neural networks (animal brains) and how the two may assist each other in developments. 

I also started the first part of tutorial 1, and I created a linear deep network of depth 1 and width 200 using pytorch.

Resources:
* [Deep Learning Intro Notes](./prep/dl_intro_notes.md)

<i>9:20 am, 100 minutes</i>

---

<b>Decoding Neural Responses Part 1</b><br>
<i>06/06/2022, 7:45 am</i>


Synopsis: I will start working through tutorial 1 of the deep learning training module.

Data: I wrote the code necessary for loading the dataset, visualizing the data, and splitting into train and test sets. I learned about the ReLU function and other non-linear activation functions. I added a ReLU layer to my deep network from last class. I also started working with the loss function and gradient descent in pyTorch.

Resources:
* [Tutorial 1 Notes](./prep/dl_tutorial1.md)
* [Tutorial 1 Code](./prep/dl_tutorial1_code.ipynb)

<i>9:20 am, 100 minutes</i>

---

<b>Decoding Neural Responses Part 2</b><br>
<i>06/06/2022, 7:45 am</i>


Synopsis: I will finish working through tutorial 1 of the deep learning training module.

Data: I successfully trained my model with the parameters given in the tutorial. I also learned about neural network expressivity and the role that depth and width have on transforming data. I also reviewed the calculations that go into gradient descent/backpropagation and the difference between gradient descent and stochastic gradient descent. Finally, I learned about convolutional neural networks.

Resources:
* [Tutorial 1 Notes](./prep/dl_tutorial1.md)
* [Tutorial 1 Code](./prep/dl_tutorial1_code.ipynb)

<i>9:20 am, 100 minutes</i>

---

<b>Jupyter Notebook Setup</b><br>
<i>06/20/2022, 9:15 am</i>


Synopsis: I will set up my Jupyter Notebook.

Data: I created and built my Jupyter Notebook with the help of the Jupyter docs. This notebook will contain both my daily log for SRP as well as all work done for the internship. I also transferred previous entries, notes, and code to my notebook.

Resources:
* [Jupyter Docs](https://jupyterbook.org/en/stable/intro.html)

<i>12:34 pm, 199 minutes</i> 

---

<b>Logistic Regression</b><br>
<i>06/20/2022, 1:45 pm</i>


Synopsis: I will learn about logistic regression.

Data: I looked into logistic regression materials, including a video lecture series by Andrew Ng on YouTube and an article published by Towards Data Science. Notable aspects of study included description of a classification problem, sigmoid activation function, cost function, and gradient descent. I initially learned about logistic regression for a standard binary classification problem before extending it to multi-class classification problems. I also published my Jupyter notebook.

Resources:
* [Logistic Regression](./prep/log_reg/logistic_regression.md)
* [Logistic Regression Lecture by Andrew Ng](https://www.youtube.com/watch?v=-EIfb6vFJzc)
* [Towards Data Science Logistic Regression Overview](https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc)
* [Interpretable ML Book by Christoph Molnar](https://christophm.github.io/interpretable-ml-book/logistic.html)

<i>5:20 pm, 215 minutes</i> 

---

<b>Logistic Regression Code</b><br>
<i>06/20/2022, 8:00 pm</i>


Synopsis: I will apply logistic regression on a dataset with Python.

Data: I imported a diabetes prediction dataset from Kaggle for logistic regression. In Python, I created functions for calculating cost, gradients, and final accuracy, as well as initializing the datasets. My code also loops through and performs gradient descent a set number of times (10000) and graphs the change in cost from iteration to iteration.

Resources:
* [Diabetes Prediction Code](./prep/log_reg/diabetes-logreg.ipynb)
* [Kaggle Dataset](https://www.kaggle.com/datasets/kandij/diabetes-dataset)

<i>9:10 pm, 70 minutes</i> 

---

<b>Logistic Regression Code Analysis</b><br>
<i>06/21/2022, 9:20 am</i>


Synopsis: I will analyze the results of my logistic regression diabetes prediction program.

Data: 

Resources:
* [Diabetes Prediction Code](./prep/log_reg/diabetes-logreg.ipynb)
* [Kaggle Dataset](https://www.kaggle.com/datasets/kandij/diabetes-dataset)

<i></i> 

---