{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from lstm_data_prep import prep\n",
    "from rnn_perm_test import iterateSeq, test_random_labels, test_random_column_labels, test_random_features\n",
    "from attention import test_model, initialize_encoder_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "with open('HCP_movie_watching.pkl','rb') as f:\n",
    "    TS = pickle.load(f)\n",
    "\n",
    "input_size = 300\n",
    "hidden_size = 32\n",
    "n_layers = 1\n",
    "seq_len = 90\n",
    "class_num = 15\n",
    "\n",
    "_, test_loader = prep(TS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anna&Megan\\Documents\\GitHub\\srp-lab-notebook\\hcp_data\\attention.py:187: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  decoder_input = torch.tensor(new_input, device=device)\n",
      "c:\\Users\\Anna&Megan\\Documents\\GitHub\\srp-lab-notebook\\hcp_data\\attention.py:189: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_loss += loss_fn(decoder_output.view(curr_batch_size,-1), torch.tensor(y[:, j, :], device=device)).item()\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'output' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Anna&Megan\\Documents\\GitHub\\srp-lab-notebook\\hcp_data\\attention-book.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Anna%26Megan/Documents/GitHub/srp-lab-notebook/hcp_data/attention-book.ipynb#ch0000002?line=6'>7</a>\u001b[0m encoder\u001b[39m.\u001b[39meval()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Anna%26Megan/Documents/GitHub/srp-lab-notebook/hcp_data/attention-book.ipynb#ch0000002?line=7'>8</a>\u001b[0m decoder\u001b[39m.\u001b[39meval()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Anna%26Megan/Documents/GitHub/srp-lab-notebook/hcp_data/attention-book.ipynb#ch0000002?line=9'>10</a>\u001b[0m attention_accuracy \u001b[39m=\u001b[39m test_model(encoder, decoder, test_loader, seq_len, loss_fn)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anna%26Megan/Documents/GitHub/srp-lab-notebook/hcp_data/attention-book.ipynb#ch0000002?line=10'>11</a>\u001b[0m attention_accuracy \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39minsert(attention_accuracy, \u001b[39m0\u001b[39m, attention_accuracy[\u001b[39m0\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anna%26Megan/Documents/GitHub/srp-lab-notebook/hcp_data/attention-book.ipynb#ch0000002?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(test_loss)\n",
      "File \u001b[1;32mc:\\Users\\Anna&Megan\\Documents\\GitHub\\srp-lab-notebook\\hcp_data\\attention.py:194\u001b[0m, in \u001b[0;36mtest_model\u001b[1;34m(encoder, decoder, test_loader, seq_len, loss_fn)\u001b[0m\n\u001b[0;32m    192\u001b[0m                 output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(new_input\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m))\n\u001b[0;32m    193\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 194\u001b[0m                 output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((output, new_input\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)), \u001b[39m1\u001b[39m)\n\u001b[0;32m    196\u001b[0m         correct, total \u001b[39m=\u001b[39m accuracy(y\u001b[39m.\u001b[39mnumpy(), output\u001b[39m.\u001b[39mnumpy(), correct, total)\n\u001b[0;32m    197\u001b[0m test_loss \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(test_loader)\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'output' referenced before assignment"
     ]
    }
   ],
   "source": [
    "encoder, attent, decoder, encoder_optimizer, decoder_optimizer, loss_fn = initialize_encoder_decoder(input_size, hidden_size, class_num)\n",
    "\n",
    "check = torch.load(\"encoder-decoder.pt\")\n",
    "encoder.load_state_dict(check[\"encoder\"])\n",
    "decoder.load_state_dict(check[\"decoder\"])\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "attention_accuracy = test_model(encoder, decoder, test_loader, seq_len, loss_fn)\n",
    "attention_accuracy = np.insert(attention_accuracy, 0, attention_accuracy[0])\n",
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_permutation_acc, _ = iterateSeq(encoder, decoder, loss_fn, TS, numSamples = 1)\n",
    "random_feature_acc, _ = test_random_features(encoder, decoder, loss_fn, TS, numSamples = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with random labels\n",
    "# random_labels_samples_accuracy, l_loss = test_random_labels(encoder, decoder, loss_fn, TS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anna&Megan\\Documents\\GitHub\\srp-lab-notebook\\hcp_data\\lstm_data_prep.py:59: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_padded = paddingArr(np.array(X_arr))\n",
      "c:\\Users\\Anna&Megan\\Documents\\GitHub\\srp-lab-notebook\\hcp_data\\attention.py:178: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  decoder_input = torch.tensor(y[:, 0, :], device=device)\n",
      "c:\\Users\\Anna&Megan\\Documents\\GitHub\\srp-lab-notebook\\hcp_data\\attention.py:189: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  decoder_input = torch.tensor(new_input, device=device)\n",
      "c:\\Users\\Anna&Megan\\Documents\\GitHub\\srp-lab-notebook\\hcp_data\\attention.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_loss += loss_fn(decoder_output.view(curr_batch_size,-1), torch.tensor(y[:, j, :], device=device)).item()\n",
      "c:\\Users\\Anna&Megan\\Documents\\GitHub\\srp-lab-notebook\\hcp_data\\attention.py:194: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  output = torch.tensor(new_input.unsqueeze(1))\n"
     ]
    }
   ],
   "source": [
    "random_batch_labels_accuracy, _ = test_random_column_labels(encoder, decoder, loss_fn, TS, numSamples = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (90,) and (89,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Anna&Megan\\Documents\\GitHub\\srp-lab-notebook\\hcp_data\\attention.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anna%26Megan/Documents/GitHub/srp-lab-notebook/hcp_data/attention.ipynb#ch0000009?line=11'>12</a>\u001b[0m \u001b[39m# Test with randomly generated label for each batch, same across time steps within batch\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anna%26Megan/Documents/GitHub/srp-lab-notebook/hcp_data/attention.ipynb#ch0000009?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, plot \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(random_batch_labels_accuracy):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Anna%26Megan/Documents/GitHub/srp-lab-notebook/hcp_data/attention.ipynb#ch0000009?line=13'>14</a>\u001b[0m     plt\u001b[39m.\u001b[39;49mplot(xAx, plot, label\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mrandom-labels-\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39mstr\u001b[39;49m(i\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anna%26Megan/Documents/GitHub/srp-lab-notebook/hcp_data/attention.ipynb#ch0000009?line=15'>16</a>\u001b[0m \u001b[39m# Test with permuted inputs\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Anna%26Megan/Documents/GitHub/srp-lab-notebook/hcp_data/attention.ipynb#ch0000009?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, plot \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(test_permutation_acc):\n",
      "File \u001b[1;32mc:\\Users\\Anna&Megan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\pyplot.py:2769\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2767\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mplot)\n\u001b[0;32m   2768\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot\u001b[39m(\u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, scaley\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> 2769\u001b[0m     \u001b[39mreturn\u001b[39;00m gca()\u001b[39m.\u001b[39mplot(\n\u001b[0;32m   2770\u001b[0m         \u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39mscalex, scaley\u001b[39m=\u001b[39mscaley,\n\u001b[0;32m   2771\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m({\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m: data} \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Anna&Megan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\axes\\_axes.py:1632\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1391\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1392\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1629\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1630\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1631\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[1;32m-> 1632\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[0;32m   1633\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[0;32m   1634\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32mc:\\Users\\Anna&Megan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\axes\\_base.py:312\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[0;32m    311\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[1;32m--> 312\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plot_args(this, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Anna&Megan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\axes\\_base.py:498\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[0;32m    495\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39myaxis\u001b[39m.\u001b[39mupdate_units(y)\n\u001b[0;32m    497\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[1;32m--> 498\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y must have same first dimension, but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    499\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhave shapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    500\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y can be no greater than 2D, but have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    502\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (90,) and (89,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare accuracies\n",
    "xAx = [i for i in range(0,90)]\n",
    "plt.plot(xAx, attention_accuracy, label=\"attention-encoder-decoder\")\n",
    "# plt.plot(xAx, attention_permutation_accuracy, label=\"permutation\")\n",
    "\"\"\"\n",
    "# Test with randomly generated label for each (batch, time step)\n",
    "for i, plot in enumerate(random_labels_samples_accuracy):\n",
    "    plot = np.insert(plot, 0, plot[0])\n",
    "    plt.plot(xAx, plot, label=\"random-\" + str(i+1))\n",
    "\"\"\"\n",
    "\n",
    "# Test with randomly generated label for each batch, same across time steps within batch\n",
    "for i, plot in enumerate(random_batch_labels_accuracy):\n",
    "    plt.plot(xAx, plot, label=\"random-labels-\" + str(i+1))\n",
    "\n",
    "# Test with permuted inputs\n",
    "for i, plot in enumerate(test_permutation_acc):\n",
    "    plt.plot(xAx, plot, label=\"permutation-\" + str(i+1))\n",
    "\n",
    "# Test with randomly generated inputs\n",
    "for i, plot in enumerate(random_feature_acc):\n",
    "    plot = np.insert (plot, 0, plot[0])\n",
    "    plt.plot(xAx, plot, label=\"\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0,1)\n",
    "plt.xlim(0,90)\n",
    "plt.title(\"Time-varying Classification Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'attention_accuracy' (ndarray)\n"
     ]
    }
   ],
   "source": [
    "%store attention_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d8d8f94dc29cf6517d9b951f40e6c965bcb2efc4a5d0d869ef8b359fa785960"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
